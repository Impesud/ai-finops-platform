# ğŸ“˜ AI FinOps Platform

ğŸ’ª To set up the AI FinOps Platform, first create a Python virtual environment using:
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

ğŸš€ Then run the FastAPI backend with:
```bash
make run
```

ğŸ“Œ You can access the live API documentation here:  
http://127.0.0.1:8000/docs

ğŸ““ To use the Jupyter Notebooks for machine learning modules, launch them with:
```bash
make notebook
```

ğŸ” Then open the following files in your browser:  
ğŸ“ˆ notebooks/01_forecasting.ipynb  
ğŸš¨ notebooks/02_anomaly_detection.ipynb  
ğŸ§¹ notebooks/03_clustering.ipynb

âœ… To run the test suite and validate backend functionality:
```bash
make test
```

ğŸ³ To containerize and run the platform with Docker:
```bash
make docker-build
make docker-up
```

ğŸ“› To stop all running Docker services:
```bash
make docker-stop
```

ğŸ©¹ To clean your local project environment and remove temporary files:
```bash
make clean
```

ğŸ“‹ Complete list of available Makefile commands for automation:
- `make init` â†’ ğŸ“¦ Create the virtual environment and install requirements  
- `make run` â†’ ğŸš€ Start the FastAPI backend server  
- `make notebook` â†’ ğŸ““ Launch the Jupyter interface  
- `make test` â†’ âœ… Execute unit and integration tests  
- `make docker-build` â†’ ğŸ³ Build Docker containers  
- `make docker-up` â†’ ğŸŸ¢ Run the application via Docker  
- `make docker-stop` â†’ ğŸ”» Stop all Docker containers  
- `make api-docs` â†’ ğŸ” Print the API documentation link  
- `make clean` â†’ ğŸ©¹ Clean up cache and temp files

ğŸ§  The platform uses FastAPI with PostgreSQL for relational storage and InfluxDB for time series data. Authentication and authorization are handled via OAuth2 and JWT tokens. The machine learning layer includes forecasting using XGBoost and statsmodels, anomaly detection with Isolation Forest and Autoencoders, clustering with KMeans, and cost-saving recommendations via Reinforcement Learning. Models are tracked and deployed using MLflow. The frontend is built using React and styled with TailwindCSS. All data is visualized using Chart.js and Recharts, with secure communication via JWT-protected API calls. Infrastructure provisioning is performed via Terraform for AWS, GCP, and Azure. The platform is deployed in Kubernetes clusters managed with Helm. Monitoring is performed using Prometheus and Grafana. Continuous integration and testing pipelines are configured using GitHub Actions.

ğŸ“‚ The repository is organized as follows:  
ğŸ“ `app/` â€“ FastAPI backend logic and APIs  
ğŸ“ `frontend/` â€“ React web dashboard with visualizations  
ğŸ“ `notebooks/` â€“ Jupyter notebooks for machine learning tasks  
ğŸ“ `infra/` â€“ Terraform modules and Helm charts  
ğŸ“ `tests/` â€“ Backend unit and integration tests  
ğŸ“ `docs/` â€“ Technical documentation and this manual  
ğŸ“ `data/` â€“ Simulated or real cloud billing datasets  
ğŸ“„ `Makefile` â€“ Task automation  
ğŸ“„ `requirements.txt` â€“ Python package list  
ğŸ“„ `docker-compose.yml` â€“ Local container setup  
ğŸ“„ `README.md` â€“ Main project summary


